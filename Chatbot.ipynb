{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtgdRkCNxbx1"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMeD70JilyF"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJEcP40Wput"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bnM_MSx1YI"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "# !pip install nlpia                                                       # Downloading the NLPIA Package.\n",
        "\n",
        "from nlpia.loaders import get_data \n",
        "import os\n",
        "from random import shuffle                                                 # Module for shuffling the Dataset.\n",
        "from IPython.display import display\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9pYizaYRFF"
      },
      "source": [
        "**Getting the Data**\n",
        "* I will use the [**Cornell Movie Dialog Dataset**](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). Using the entire Cornell Movie Dialog Dataset can be computationally intensive because a few sequences have more than 2000 tokens. I will use **NLPIA** Package to load the Cornell Movie Dialog Dataset and I will pre process the Dialog Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-PreqwYDFI"
      },
      "source": [
        "#@ Getting the Data:\n",
        "df = get_data(\"moviedialog\")                                                # Accessing the Cornell Movie Dialog Corpus.\n",
        "\n",
        "#@ Processing the Data:\n",
        "input_texts = []                                                            # The array holds the input text from the Corpus.\n",
        "target_texts = []                                                           # The array holds the target text from the Corpus.\n",
        "input_vocabulary = set()                                                    # Holds the seen characters in the input text.\n",
        "output_vocabulary = set()                                                   # Holds the seen characters in the target txt.\n",
        "\n",
        "start_token = \"\\t\"                                                          # Target sequence is annotated with start Token.\n",
        "stop_token = \"\\n\"                                                           # Target sequence is annotated with sop Token.\n",
        "max_training_samples = min(25000, len(df) - 1)                              # Defines the lines used for Training.\n",
        "\n",
        "for input_text, target_text in zip(df.statement, df.reply):\n",
        "  target_text = start_token + target_text + stop_token                      # The Target Text needs to be wrapped with start and stop tokens.\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  \n",
        "  #@ Compiling the Vocabulary set:\n",
        "  for char in input_text:\n",
        "    if char not in input_vocabulary:\n",
        "      input_vocabulary.add(char)\n",
        "  \n",
        "  for char in target_text:\n",
        "    if char not in output_vocabulary:\n",
        "      output_vocabulary.add(char)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29qMwBjZjPXc"
      },
      "source": [
        "**Building the Character Dictionary**\n",
        "* I will convert each characters of the Input and Target Texts into one hot vectors that represent each characters. In order to generate one hot vectors I will generate token dictionaries where every character is mapped to an index. I will also generate the reverse dictionaries which will be used to convert generated index into characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrnA7fjMb4vd"
      },
      "source": [
        "#@ Sorting the List of Characters:\n",
        "input_vocabulary = sorted(input_vocabulary)\n",
        "output_vocabulary = sorted(output_vocabulary)\n",
        "\n",
        "#@ Calculating the Maximum number of Unique Characters:\n",
        "input_vocab_size = len(input_vocabulary)\n",
        "output_vocab_size = len(output_vocabulary)\n",
        "\n",
        "#@ Determining the Maximum number of Sequence Tokens:\n",
        "max_encoder_seq_length = [len(txt) for txt in input_texts]\n",
        "max_decoder_seq_length = [len(txt) for txt in target_texts]\n",
        "\n",
        "#@ Creating the Token Dictionaries:\n",
        "input_token_index = dict([(char, i) for i,char in enumerate(input_vocabulary)])\n",
        "target_token_index = dict([(char, i) for i,char in enumerate(output_vocabulary)])\n",
        "\n",
        "#@ Creating the Reverse Token Dictionaries:\n",
        "reverse_input_char_index = dict((i, char) for char,i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char,i in target_token_index.items())"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}