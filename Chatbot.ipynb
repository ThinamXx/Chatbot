{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtgdRkCNxbx1"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMeD70JilyF"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJEcP40Wput"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bnM_MSx1YI"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "# !pip install nlpia                                                       # Downloading the NLPIA Package.\n",
        "\n",
        "import numpy as np                                                         # Module for matrix multiplication.\n",
        "from nlpia.loaders import get_data \n",
        "import os\n",
        "from random import shuffle                                                 # Module for shuffling the Dataset.\n",
        "from IPython.display import display\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9pYizaYRFF"
      },
      "source": [
        "**Getting the Data**\n",
        "* I will use the [**Cornell Movie Dialog Dataset**](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). Using the entire Cornell Movie Dialog Dataset can be computationally intensive because a few sequences have more than 2000 tokens. I will use **NLPIA** Package to load the Cornell Movie Dialog Dataset and I will pre process the Dialog Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-PreqwYDFI",
        "outputId": "b8353c0f-f39f-4bf7-f557-95d2eca3f8bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#@ Getting the Data:\n",
        "df = get_data(\"moviedialog\")                                                # Accessing the Cornell Movie Dialog Corpus.\n",
        "\n",
        "#@ Processing the Data:\n",
        "input_texts = []                                                            # The array holds the input text from the Corpus.\n",
        "target_texts = []                                                           # The array holds the target text from the Corpus.\n",
        "input_vocabulary = set()                                                    # Holds the seen characters in the input text.\n",
        "output_vocabulary = set()                                                   # Holds the seen characters in the target txt.\n",
        "\n",
        "start_token = \"\\t\"                                                          # Target sequence is annotated with start Token.\n",
        "stop_token = \"\\n\"                                                           # Target sequence is annotated with sop Token.\n",
        "max_training_samples = min(25000, len(df) - 1)                              # Defines the lines used for Training.\n",
        "\n",
        "for input_text, target_text in zip(df.statement, df.reply):\n",
        "  target_text = start_token + target_text + stop_token                      # The Target Text needs to be wrapped with start and stop tokens.\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  \n",
        "  #@ Compiling the Vocabulary set:\n",
        "  for char in input_text:\n",
        "    if char not in input_vocabulary:\n",
        "      input_vocabulary.add(char)\n",
        "  \n",
        "  for char in target_text:\n",
        "    if char not in output_vocabulary:\n",
        "      output_vocabulary.add(char)\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "display(f\"Number of samples: {len(input_texts)}\")\n",
        "display(f\"Number of unique Input Tokens: {len(input_vocabulary)}\")\n",
        "display(f\"Number of unique Output Tokens: {len(output_vocabulary)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of samples: 64350'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of unique Input Tokens: 44'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of unique Output Tokens: 46'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29qMwBjZjPXc"
      },
      "source": [
        "**Building the Character Dictionary**\n",
        "* I will convert each characters of the Input and Target Texts into one hot vectors that represent each characters. In order to generate one hot vectors I will generate token dictionaries where every character is mapped to an index. I will also generate the reverse dictionaries which will be used to convert generated index into characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrnA7fjMb4vd",
        "outputId": "bf380256-3655-43b3-f924-05207530411f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#@ Sorting the List of Characters:\n",
        "input_vocabulary = sorted(list(input_vocabulary))\n",
        "output_vocabulary = sorted(list(output_vocabulary))\n",
        "\n",
        "#@ Calculating the Maximum number of Unique Characters:\n",
        "input_vocab_size = len(input_vocabulary)\n",
        "output_vocab_size = len(output_vocabulary)\n",
        "\n",
        "#@ Determining the Maximum number of Sequence Tokens:\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#@ Creating the Token Dictionaries:\n",
        "input_token_index = dict([(char, i) for i,char in enumerate(input_vocabulary)])\n",
        "target_token_index = dict([(char, i) for i,char in enumerate(output_vocabulary)])\n",
        "\n",
        "#@ Creating the Reverse Token Dictionaries:\n",
        "reverse_input_char_index = dict((i, char) for char,i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char,i in target_token_index.items())\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "display(f\"Maximum sequence length for Inputs: {max_encoder_seq_length}\")\n",
        "display(f\"Maximum sequence length for Outputs: {max_decoder_seq_length}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maximum sequence length for Inputs: 100'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maximum sequence length for Outputs: 102'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fIX28MrwXO"
      },
      "source": [
        "**Generating One Hot Encoded Training sets**\n",
        "* Now, I will convert the input and target text into one hot Encoded Tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkpcZ8YssSOe"
      },
      "source": [
        "#@ Creating character sequence Encoder and Decoder Training Set:\n",
        "\n",
        "#@ Initializing the Tensors with zeros:\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, input_vocab_size), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype=\"float32\")\n",
        "\n",
        "#@ Looping over the Training Samples:\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "  #@ Looping over each character of each Samples:\n",
        "  for t, char in enumerate(input_text):\n",
        "    encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "  for t, char in enumerate(target_text):\n",
        "    decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "    if t > 0:\n",
        "      decoder_target_data[i, t-1, target_token_index[char]] = 1.\n",
        "  decoder_input_data[i, t+1:, target_token_index[\" \"]] = 1.\n",
        "  decoder_target_data[i, t:, target_token_index[\" \"]] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6_CLPRzzLA"
      },
      "source": [
        "### **Sequence to Sequence Chatbot**\n",
        "* I have completed all the Training set preparations by performing the tasks such as Converting the preprocessed Corpus into Input and Target Samples and creating Index Dictionaries and converting the Samples into One hot Tensors. Now, I will train the Sequence to sequence Chatbot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLGAaDXtyvIP"
      },
      "source": [
        "#@ Parameters of LSTM Neural Networks:\n",
        "batch_size = 64                                 # Number of samples shown to the network before updating the weights.\n",
        "epochs = 100                                    # Number of times for passing the Training.\n",
        "num_neurons = 256                               # Setting the number of neuron dimensions to 256.\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbDzIUUk5JDX",
        "outputId": "1ab7da14-db37-43b4-cf85-401be6ba88ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@ Sequence to Sequence Encoder Decoder Network:\n",
        "\n",
        "#@ Creating the Thought Encoder using Keras Functional API:\n",
        "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
        "encoder = LSTM(num_neurons, return_state=True)                                          # Returning the internal state of LSTM.\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs) \n",
        "encoder_states = [state_h, state_c]                                                     # First value of LSTM is the Output.\n",
        "\n",
        "#@ Creating the Thought Decoder using Keras Functional API:\n",
        "decoder_inputs = Input(shape=(None, output_vocab_size))\n",
        "decoder_lstm = LSTM(\n",
        "    num_neurons, return_sequences=True, return_state=True\n",
        ")\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)      # Passing initial state to the LSTM Layer.\n",
        "decoder_dense = Dense(output_vocab_size, activation=\"softmax\")                          \n",
        "decoder_outputs = decoder_dense(decoder_outputs)                                        # Passing the output to the Softmax Layer.\n",
        "\n",
        "#@ Creating the Sequence to Sequence Neural Network Model:\n",
        "model = Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs\n",
        ")\n",
        "\n",
        "#@ Compiling the Sequence to Sequence Neural Network Model:\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"categorical_crossentropy\",                                                    # Using Categorical Crossentropy.\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Sequence to Sequence Neural Network Model:\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data], \n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2                                                               # 10% of Samples are splitted for Validation.\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "805/805 [==============================] - 19s 24ms/step - loss: 0.8816 - accuracy: 0.7612 - val_loss: 0.6955 - val_accuracy: 0.7924\n",
            "Epoch 2/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.6123 - accuracy: 0.8159 - val_loss: 0.6006 - val_accuracy: 0.8182\n",
            "Epoch 3/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.5492 - accuracy: 0.8328 - val_loss: 0.5555 - val_accuracy: 0.8302\n",
            "Epoch 4/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.5174 - accuracy: 0.8411 - val_loss: 0.5340 - val_accuracy: 0.8362\n",
            "Epoch 5/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4976 - accuracy: 0.8463 - val_loss: 0.5177 - val_accuracy: 0.8407\n",
            "Epoch 6/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4838 - accuracy: 0.8502 - val_loss: 0.5074 - val_accuracy: 0.8436\n",
            "Epoch 7/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4735 - accuracy: 0.8530 - val_loss: 0.5021 - val_accuracy: 0.8448\n",
            "Epoch 8/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4654 - accuracy: 0.8552 - val_loss: 0.4954 - val_accuracy: 0.8473\n",
            "Epoch 9/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4587 - accuracy: 0.8571 - val_loss: 0.4902 - val_accuracy: 0.8487\n",
            "Epoch 10/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4530 - accuracy: 0.8586 - val_loss: 0.4868 - val_accuracy: 0.8495\n",
            "Epoch 11/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4481 - accuracy: 0.8599 - val_loss: 0.4841 - val_accuracy: 0.8502\n",
            "Epoch 12/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4440 - accuracy: 0.8611 - val_loss: 0.4831 - val_accuracy: 0.8507\n",
            "Epoch 13/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4402 - accuracy: 0.8620 - val_loss: 0.4812 - val_accuracy: 0.8513\n",
            "Epoch 14/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4367 - accuracy: 0.8631 - val_loss: 0.4799 - val_accuracy: 0.8515\n",
            "Epoch 15/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4335 - accuracy: 0.8638 - val_loss: 0.4787 - val_accuracy: 0.8519\n",
            "Epoch 16/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4307 - accuracy: 0.8648 - val_loss: 0.4788 - val_accuracy: 0.8522\n",
            "Epoch 17/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4280 - accuracy: 0.8654 - val_loss: 0.4789 - val_accuracy: 0.8520\n",
            "Epoch 18/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4256 - accuracy: 0.8660 - val_loss: 0.4781 - val_accuracy: 0.8525\n",
            "Epoch 19/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4234 - accuracy: 0.8667 - val_loss: 0.4772 - val_accuracy: 0.8524\n",
            "Epoch 20/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4212 - accuracy: 0.8672 - val_loss: 0.4779 - val_accuracy: 0.8525\n",
            "Epoch 21/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4192 - accuracy: 0.8678 - val_loss: 0.4779 - val_accuracy: 0.8528\n",
            "Epoch 22/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4173 - accuracy: 0.8684 - val_loss: 0.4788 - val_accuracy: 0.8523\n",
            "Epoch 23/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4155 - accuracy: 0.8689 - val_loss: 0.4777 - val_accuracy: 0.8520\n",
            "Epoch 24/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4138 - accuracy: 0.8693 - val_loss: 0.4792 - val_accuracy: 0.8524\n",
            "Epoch 25/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4122 - accuracy: 0.8698 - val_loss: 0.4803 - val_accuracy: 0.8524\n",
            "Epoch 26/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.4106 - accuracy: 0.8702 - val_loss: 0.4810 - val_accuracy: 0.8524\n",
            "Epoch 27/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4092 - accuracy: 0.8706 - val_loss: 0.4805 - val_accuracy: 0.8524\n",
            "Epoch 28/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4079 - accuracy: 0.8710 - val_loss: 0.4820 - val_accuracy: 0.8520\n",
            "Epoch 29/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4065 - accuracy: 0.8715 - val_loss: 0.4812 - val_accuracy: 0.8524\n",
            "Epoch 30/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4051 - accuracy: 0.8718 - val_loss: 0.4820 - val_accuracy: 0.8520\n",
            "Epoch 31/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4039 - accuracy: 0.8722 - val_loss: 0.4827 - val_accuracy: 0.8521\n",
            "Epoch 32/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4028 - accuracy: 0.8725 - val_loss: 0.4839 - val_accuracy: 0.8513\n",
            "Epoch 33/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.4034 - accuracy: 0.8724 - val_loss: 0.4844 - val_accuracy: 0.8519\n",
            "Epoch 34/100\n",
            "805/805 [==============================] - 18s 22ms/step - loss: 0.4005 - accuracy: 0.8731 - val_loss: 0.4845 - val_accuracy: 0.8521\n",
            "Epoch 35/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3995 - accuracy: 0.8734 - val_loss: 0.4857 - val_accuracy: 0.8515\n",
            "Epoch 36/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3984 - accuracy: 0.8737 - val_loss: 0.4867 - val_accuracy: 0.8517\n",
            "Epoch 37/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3974 - accuracy: 0.8740 - val_loss: 0.4869 - val_accuracy: 0.8513\n",
            "Epoch 38/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3965 - accuracy: 0.8743 - val_loss: 0.4885 - val_accuracy: 0.8514\n",
            "Epoch 39/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3956 - accuracy: 0.8745 - val_loss: 0.4885 - val_accuracy: 0.8514\n",
            "Epoch 40/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3946 - accuracy: 0.8749 - val_loss: 0.4893 - val_accuracy: 0.8515\n",
            "Epoch 41/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3937 - accuracy: 0.8751 - val_loss: 0.4913 - val_accuracy: 0.8507\n",
            "Epoch 42/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3930 - accuracy: 0.8752 - val_loss: 0.4922 - val_accuracy: 0.8508\n",
            "Epoch 43/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3925 - accuracy: 0.8755 - val_loss: 0.4928 - val_accuracy: 0.8501\n",
            "Epoch 44/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3914 - accuracy: 0.8758 - val_loss: 0.4936 - val_accuracy: 0.8507\n",
            "Epoch 45/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3907 - accuracy: 0.8760 - val_loss: 0.4926 - val_accuracy: 0.8503\n",
            "Epoch 46/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3899 - accuracy: 0.8762 - val_loss: 0.4938 - val_accuracy: 0.8504\n",
            "Epoch 47/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3894 - accuracy: 0.8765 - val_loss: 0.4945 - val_accuracy: 0.8506\n",
            "Epoch 48/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3886 - accuracy: 0.8766 - val_loss: 0.4957 - val_accuracy: 0.8505\n",
            "Epoch 49/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3879 - accuracy: 0.8767 - val_loss: 0.4962 - val_accuracy: 0.8502\n",
            "Epoch 50/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3873 - accuracy: 0.8771 - val_loss: 0.4961 - val_accuracy: 0.8504\n",
            "Epoch 51/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3869 - accuracy: 0.8772 - val_loss: 0.4967 - val_accuracy: 0.8502\n",
            "Epoch 52/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3862 - accuracy: 0.8774 - val_loss: 0.4983 - val_accuracy: 0.8499\n",
            "Epoch 53/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3856 - accuracy: 0.8774 - val_loss: 0.4978 - val_accuracy: 0.8498\n",
            "Epoch 54/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3850 - accuracy: 0.8776 - val_loss: 0.4990 - val_accuracy: 0.8495\n",
            "Epoch 55/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3850 - accuracy: 0.8778 - val_loss: 0.4996 - val_accuracy: 0.8498\n",
            "Epoch 56/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3841 - accuracy: 0.8779 - val_loss: 0.4998 - val_accuracy: 0.8496\n",
            "Epoch 57/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3836 - accuracy: 0.8780 - val_loss: 0.4999 - val_accuracy: 0.8496\n",
            "Epoch 58/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3832 - accuracy: 0.8781 - val_loss: 0.5002 - val_accuracy: 0.8497\n",
            "Epoch 59/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3827 - accuracy: 0.8784 - val_loss: 0.5014 - val_accuracy: 0.8494\n",
            "Epoch 60/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3823 - accuracy: 0.8785 - val_loss: 0.5039 - val_accuracy: 0.8492\n",
            "Epoch 61/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3819 - accuracy: 0.8786 - val_loss: 0.5022 - val_accuracy: 0.8491\n",
            "Epoch 62/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3815 - accuracy: 0.8787 - val_loss: 0.5047 - val_accuracy: 0.8488\n",
            "Epoch 63/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3811 - accuracy: 0.8788 - val_loss: 0.5025 - val_accuracy: 0.8496\n",
            "Epoch 64/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3808 - accuracy: 0.8788 - val_loss: 0.5050 - val_accuracy: 0.8488\n",
            "Epoch 65/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3804 - accuracy: 0.8789 - val_loss: 0.5051 - val_accuracy: 0.8490\n",
            "Epoch 66/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3852 - accuracy: 0.8778 - val_loss: 0.5205 - val_accuracy: 0.8453\n",
            "Epoch 67/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3881 - accuracy: 0.8767 - val_loss: 0.5037 - val_accuracy: 0.8487\n",
            "Epoch 68/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3830 - accuracy: 0.8781 - val_loss: 0.5021 - val_accuracy: 0.8495\n",
            "Epoch 69/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3802 - accuracy: 0.8790 - val_loss: 0.5044 - val_accuracy: 0.8492\n",
            "Epoch 70/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3795 - accuracy: 0.8793 - val_loss: 0.5054 - val_accuracy: 0.8484\n",
            "Epoch 71/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3790 - accuracy: 0.8794 - val_loss: 0.5064 - val_accuracy: 0.8486\n",
            "Epoch 72/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3787 - accuracy: 0.8795 - val_loss: 0.5060 - val_accuracy: 0.8487\n",
            "Epoch 73/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3783 - accuracy: 0.8796 - val_loss: 0.5074 - val_accuracy: 0.8486\n",
            "Epoch 74/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3780 - accuracy: 0.8796 - val_loss: 0.5084 - val_accuracy: 0.8484\n",
            "Epoch 75/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3777 - accuracy: 0.8797 - val_loss: 0.5073 - val_accuracy: 0.8485\n",
            "Epoch 76/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3775 - accuracy: 0.8798 - val_loss: 0.5077 - val_accuracy: 0.8482\n",
            "Epoch 77/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3772 - accuracy: 0.8799 - val_loss: 0.5091 - val_accuracy: 0.8483\n",
            "Epoch 78/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3769 - accuracy: 0.8799 - val_loss: 0.5090 - val_accuracy: 0.8483\n",
            "Epoch 79/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3767 - accuracy: 0.8801 - val_loss: 0.5095 - val_accuracy: 0.8480\n",
            "Epoch 80/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3764 - accuracy: 0.8802 - val_loss: 0.5093 - val_accuracy: 0.8483\n",
            "Epoch 81/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3765 - accuracy: 0.8803 - val_loss: 0.5098 - val_accuracy: 0.8483\n",
            "Epoch 82/100\n",
            "805/805 [==============================] - 18s 23ms/step - loss: 0.3759 - accuracy: 0.8804 - val_loss: 0.5102 - val_accuracy: 0.8481\n",
            "Epoch 83/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3756 - accuracy: 0.8805 - val_loss: 0.5123 - val_accuracy: 0.8479\n",
            "Epoch 84/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3755 - accuracy: 0.8805 - val_loss: 0.5103 - val_accuracy: 0.8485\n",
            "Epoch 85/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3752 - accuracy: 0.8806 - val_loss: 0.5112 - val_accuracy: 0.8481\n",
            "Epoch 86/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3801 - accuracy: 0.8794 - val_loss: 0.5098 - val_accuracy: 0.8484\n",
            "Epoch 87/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3752 - accuracy: 0.8807 - val_loss: 0.5109 - val_accuracy: 0.8486\n",
            "Epoch 88/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3748 - accuracy: 0.8807 - val_loss: 0.5112 - val_accuracy: 0.8481\n",
            "Epoch 89/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3744 - accuracy: 0.8808 - val_loss: 0.5108 - val_accuracy: 0.8479\n",
            "Epoch 90/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3741 - accuracy: 0.8810 - val_loss: 0.5123 - val_accuracy: 0.8482\n",
            "Epoch 91/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3740 - accuracy: 0.8810 - val_loss: 0.5120 - val_accuracy: 0.8483\n",
            "Epoch 92/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3740 - accuracy: 0.8811 - val_loss: 0.5116 - val_accuracy: 0.8483\n",
            "Epoch 93/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3736 - accuracy: 0.8812 - val_loss: 0.5138 - val_accuracy: 0.8476\n",
            "Epoch 94/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3733 - accuracy: 0.8811 - val_loss: 0.5133 - val_accuracy: 0.8479\n",
            "Epoch 95/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3731 - accuracy: 0.8812 - val_loss: 0.5136 - val_accuracy: 0.8478\n",
            "Epoch 96/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3729 - accuracy: 0.8813 - val_loss: 0.5137 - val_accuracy: 0.8475\n",
            "Epoch 97/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3728 - accuracy: 0.8814 - val_loss: 0.5141 - val_accuracy: 0.8479\n",
            "Epoch 98/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3725 - accuracy: 0.8815 - val_loss: 0.5139 - val_accuracy: 0.8482\n",
            "Epoch 99/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3724 - accuracy: 0.8815 - val_loss: 0.5137 - val_accuracy: 0.8481\n",
            "Epoch 100/100\n",
            "805/805 [==============================] - 19s 23ms/step - loss: 0.3721 - accuracy: 0.8816 - val_loss: 0.5152 - val_accuracy: 0.8479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6a5ac6f9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJhfUd1fxhr"
      },
      "source": [
        "**Saving the Sequence to Sequence Chatbot Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsHhqKNDgBMm",
        "outputId": "87bb96d6-7806-4b1b-9c0a-b0797ae67fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@ Saving the Sequence to Sequence Network Model:\n",
        "model_structure = model.to_json()\n",
        "with open(\"sequence_model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"sequence_model.h5\")\n",
        "print(\"Model saved successful!!\")\n",
        "\n",
        "model.load_weights(\"sequence_model.h5\")                         # Loading the saved Model."
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved successful!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5lEyWE0SOmw"
      },
      "source": [
        "**Assembling the Model for Sequence Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2gQBZG8XStP"
      },
      "source": [
        "#@ Creating the Response Generator Model:\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "thought_input = [Input(shape=(num_neurons,)), Input(shape=(num_neurons,))]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=thought_input)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#@ Creating the Model:\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_inputs] + thought_input,\n",
        "    outputs=[decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVpilaALptgE"
      },
      "source": [
        "### **Predicting the Sequence**\n",
        "* I will define a Function for generating the Response of the Chatbot. This Function is the heart of Response Generation of the Chatbot which accepts one hot encoded input sequence, generates the Thought Vector and the Thought Vector generates the appropriate response by using the **Trained Network**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljiCePksBqX"
      },
      "source": [
        "#@ Building the Character Based Translator:\n",
        "def decode_sequence(input_seq):\n",
        "  #@ Generating the Thought Vector:\n",
        "  thought = encoder_model.predict(input_seq)\n",
        "\n",
        "  target_seq = np.zeros((1, 1, output_vocab_size))                           # Initializing it as a Zero Tensor.\n",
        "  target_seq[0, 0, target_token_index[start_token]] = 1.                     # First Input Token to the decoder is the input token.\n",
        "\n",
        "  stop_condition = False\n",
        "  generated_sequence = \" \"\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + thought)     # Passing the generated Token and latest state to the Decoder.\n",
        "    generated_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "    generated_char = reverse_target_char_index[generated_token_idx]\n",
        "    generated_sequence += generated_char\n",
        "    if (generated_char == stop_token or \n",
        "        len(generated_sequence) > max_decoder_seq_length):\n",
        "      stop_condition = True                                                 # Setting the condition to True will stop the Loop.\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, output_vocab_size))\n",
        "    target_seq[0, 0, generated_token_idx] = 1.\n",
        "    thought = [h, c]                                                        # Updating the Thought Vector.\n",
        "\n",
        "  return generated_sequence"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDC_1pd8uYX"
      },
      "source": [
        "**Generating the Response**\n",
        "* Now, I will define a helper function to convert the Input String into a reply for the Chatbot to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKVIBZp4WVM"
      },
      "source": [
        "def Response(input_text):\n",
        "  input_seq = np.zeros((1, max_encoder_seq_length, input_vocab_size), dtype=\"float32\")\n",
        "  for t, char in enumerate(input_text):\n",
        "    input_seq[0, t, input_token_index[char]] = 1.\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(\"T2 Reply:\", decoded_sentence)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPrjC-3qD1td",
        "outputId": "4e23b589-134e-4180-dd24-88ebe5d2fea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Response(\"do you like coffee?\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T2 Reply:  i don't know. i don't know.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}