{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtgdRkCNxbx1"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMeD70JilyF"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJEcP40Wput"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bnM_MSx1YI"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "# !pip install nlpia                                                       # Downloading the NLPIA Package.\n",
        "\n",
        "import numpy as np                                                         # Module for matrix multiplication.\n",
        "from nlpia.loaders import get_data \n",
        "import os\n",
        "from random import shuffle                                                 # Module for shuffling the Dataset.\n",
        "from IPython.display import display\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9pYizaYRFF"
      },
      "source": [
        "**Getting the Data**\n",
        "* I will use the [**Cornell Movie Dialog Dataset**](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). Using the entire Cornell Movie Dialog Dataset can be computationally intensive because a few sequences have more than 2000 tokens. I will use **NLPIA** Package to load the Cornell Movie Dialog Dataset and I will pre process the Dialog Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-PreqwYDFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c7d17c33-8f99-45eb-9f61-1d7f980a1abe"
      },
      "source": [
        "#@ Getting the Data:\n",
        "df = get_data(\"moviedialog\")                                                # Accessing the Cornell Movie Dialog Corpus.\n",
        "\n",
        "#@ Processing the Data:\n",
        "input_texts = []                                                            # The array holds the input text from the Corpus.\n",
        "target_texts = []                                                           # The array holds the target text from the Corpus.\n",
        "input_vocabulary = set()                                                    # Holds the seen characters in the input text.\n",
        "output_vocabulary = set()                                                   # Holds the seen characters in the target txt.\n",
        "\n",
        "start_token = \"\\t\"                                                          # Target sequence is annotated with start Token.\n",
        "stop_token = \"\\n\"                                                           # Target sequence is annotated with sop Token.\n",
        "max_training_samples = min(25000, len(df) - 1)                              # Defines the lines used for Training.\n",
        "\n",
        "for input_text, target_text in zip(df.statement, df.reply):\n",
        "  target_text = start_token + target_text + stop_token                      # The Target Text needs to be wrapped with start and stop tokens.\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  \n",
        "  #@ Compiling the Vocabulary set:\n",
        "  for char in input_text:\n",
        "    if char not in input_vocabulary:\n",
        "      input_vocabulary.add(char)\n",
        "  \n",
        "  for char in target_text:\n",
        "    if char not in output_vocabulary:\n",
        "      output_vocabulary.add(char)\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "display(f\"Number of samples: {len(input_texts)}\")\n",
        "display(f\"Number of unique Input Tokens: {len(input_vocabulary)}\")\n",
        "display(f\"Number of unique Output Tokens: {len(output_vocabulary)}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of samples: 64350'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of unique Input Tokens: 44'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of unique Output Tokens: 46'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29qMwBjZjPXc"
      },
      "source": [
        "**Building the Character Dictionary**\n",
        "* I will convert each characters of the Input and Target Texts into one hot vectors that represent each characters. In order to generate one hot vectors I will generate token dictionaries where every character is mapped to an index. I will also generate the reverse dictionaries which will be used to convert generated index into characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrnA7fjMb4vd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f3be568b-7d33-4bca-c0ab-8ec68d9f88f1"
      },
      "source": [
        "#@ Sorting the List of Characters:\n",
        "input_vocabulary = sorted(input_vocabulary)\n",
        "output_vocabulary = sorted(output_vocabulary)\n",
        "\n",
        "#@ Calculating the Maximum number of Unique Characters:\n",
        "input_vocab_size = len(input_vocabulary)\n",
        "output_vocab_size = len(output_vocabulary)\n",
        "\n",
        "#@ Determining the Maximum number of Sequence Tokens:\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#@ Creating the Token Dictionaries:\n",
        "input_token_index = dict([(char, i) for i,char in enumerate(input_vocabulary)])\n",
        "target_token_index = dict([(char, i) for i,char in enumerate(output_vocabulary)])\n",
        "\n",
        "#@ Creating the Reverse Token Dictionaries:\n",
        "reverse_input_char_index = dict((i, char) for char,i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char,i in target_token_index.items())\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "display(f\"Maximum sequence length for Inputs: {max_encoder_seq_length}\")\n",
        "display(f\"Maximum sequence length for Outputs: {max_decoder_seq_length}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maximum sequence length for Inputs: 100'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maximum sequence length for Outputs: 102'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fIX28MrwXO"
      },
      "source": [
        "**Generating One Hot Encoded Training sets**\n",
        "* Now, I will convert the input and target text into one hot Encoded Tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkpcZ8YssSOe"
      },
      "source": [
        "#@ Creating character sequence Encoder and Decoder Training Set:\n",
        "\n",
        "#@ Initializing the Tensors with zeros:\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, input_vocab_size), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype=\"float32\")\n",
        "\n",
        "#@ Looping over the Training Samples:\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "  #@ Looping over each character of each Samples:\n",
        "  for t, char in enumerate(input_text):\n",
        "    encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "  for t, char in enumerate(target_text):\n",
        "    decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "    if t > 0:\n",
        "      decoder_target_data[i, t-1, target_token_index[char]] = 1.\n",
        "  # decoder_input_data[i, t+1:, target_token_index[\" \"]] = 1.\n",
        "  # decoder_target_data[i, t:, target_token_index[\" \"]] = 1."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6_CLPRzzLA"
      },
      "source": [
        "### **Sequence to Sequence Chatbot**\n",
        "* I have completed all the Training set preparations by performing the tasks such as Converting the preprocessed Corpus into Input and Target Samples and creating Index Dictionaries and converting the Samples into One hot Tensors. Now, I will train the Sequence to sequence Chatbot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLGAaDXtyvIP"
      },
      "source": [
        "#@ Parameters of LSTM Neural Networks:\n",
        "batch_size = 64                                 # Number of samples shown to the network before updating the weights.\n",
        "epochs = 100                                    # Number of times for passing the Training.\n",
        "num_neurons = 256                               # Setting the number of neuron dimensions to 256."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbDzIUUk5JDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276b332d-45db-47d0-9f71-6d9ebd534e08"
      },
      "source": [
        "#@ Sequence to Sequence Encoder Decoder Network:\n",
        "\n",
        "#@ Creating the Thought Encoder using Keras Functional API:\n",
        "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
        "encoder = LSTM(num_neurons, return_state=True)                                          # Returning the internal state of LSTM.\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs) \n",
        "encoder_states = [state_h, state_c]                                                     # First value of LSTM is the Output.\n",
        "\n",
        "#@ Creating the Thought Decoder using Keras Functional API:\n",
        "decoder_inputs = Input(shape=(None, output_vocab_size))\n",
        "decoder_lstm = LSTM(\n",
        "    num_neurons, return_sequences=True, return_state=True\n",
        ")\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)      # Passing initial state to the LSTM Layer.\n",
        "decoder_dense = Dense(output_vocab_size, activation=\"softmax\")                          \n",
        "decoder_outputs = decoder_dense(decoder_outputs)                                        # Passing the output to the Softmax Layer.\n",
        "\n",
        "#@ Creating the Sequence to Sequence Neural Network Model:\n",
        "model = Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs\n",
        ")\n",
        "\n",
        "#@ Compiling the Sequence to Sequence Neural Network Model:\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"categorical_crossentropy\",                                                    # Using Categorical Crossentropy.\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Sequence to Sequence Neural Network Model:\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data], \n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1                                                               # 10% of Samples are splitted for Validation.\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.7601 - accuracy: 0.1204 - val_loss: 0.6517 - val_accuracy: 0.1539\n",
            "Epoch 2/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.5912 - accuracy: 0.1620 - val_loss: 0.5731 - val_accuracy: 0.1745\n",
            "Epoch 3/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.5392 - accuracy: 0.1760 - val_loss: 0.5408 - val_accuracy: 0.1834\n",
            "Epoch 4/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.5114 - accuracy: 0.1833 - val_loss: 0.5183 - val_accuracy: 0.1893\n",
            "Epoch 5/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4937 - accuracy: 0.1881 - val_loss: 0.5057 - val_accuracy: 0.1930\n",
            "Epoch 6/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4813 - accuracy: 0.1914 - val_loss: 0.4963 - val_accuracy: 0.1958\n",
            "Epoch 7/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4717 - accuracy: 0.1941 - val_loss: 0.4906 - val_accuracy: 0.1968\n",
            "Epoch 8/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4641 - accuracy: 0.1961 - val_loss: 0.4849 - val_accuracy: 0.1984\n",
            "Epoch 9/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4580 - accuracy: 0.1978 - val_loss: 0.4824 - val_accuracy: 0.1995\n",
            "Epoch 10/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4527 - accuracy: 0.1992 - val_loss: 0.4791 - val_accuracy: 0.2000\n",
            "Epoch 11/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4482 - accuracy: 0.2005 - val_loss: 0.4761 - val_accuracy: 0.2009\n",
            "Epoch 12/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4442 - accuracy: 0.2015 - val_loss: 0.4751 - val_accuracy: 0.2014\n",
            "Epoch 13/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4407 - accuracy: 0.2025 - val_loss: 0.4735 - val_accuracy: 0.2019\n",
            "Epoch 14/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4375 - accuracy: 0.2034 - val_loss: 0.4721 - val_accuracy: 0.2024\n",
            "Epoch 15/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4345 - accuracy: 0.2042 - val_loss: 0.4711 - val_accuracy: 0.2025\n",
            "Epoch 16/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4319 - accuracy: 0.2049 - val_loss: 0.4713 - val_accuracy: 0.2025\n",
            "Epoch 17/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4295 - accuracy: 0.2055 - val_loss: 0.4703 - val_accuracy: 0.2028\n",
            "Epoch 18/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4273 - accuracy: 0.2061 - val_loss: 0.4697 - val_accuracy: 0.2031\n",
            "Epoch 19/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4252 - accuracy: 0.2067 - val_loss: 0.4694 - val_accuracy: 0.2031\n",
            "Epoch 20/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4231 - accuracy: 0.2072 - val_loss: 0.4693 - val_accuracy: 0.2034\n",
            "Epoch 21/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4213 - accuracy: 0.2079 - val_loss: 0.4695 - val_accuracy: 0.2034\n",
            "Epoch 22/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4196 - accuracy: 0.2082 - val_loss: 0.4698 - val_accuracy: 0.2033\n",
            "Epoch 23/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4179 - accuracy: 0.2087 - val_loss: 0.4695 - val_accuracy: 0.2033\n",
            "Epoch 24/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4163 - accuracy: 0.2092 - val_loss: 0.4695 - val_accuracy: 0.2032\n",
            "Epoch 25/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4154 - accuracy: 0.2094 - val_loss: 0.4705 - val_accuracy: 0.2035\n",
            "Epoch 26/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4135 - accuracy: 0.2100 - val_loss: 0.4707 - val_accuracy: 0.2035\n",
            "Epoch 27/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4121 - accuracy: 0.2104 - val_loss: 0.4711 - val_accuracy: 0.2032\n",
            "Epoch 28/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4108 - accuracy: 0.2107 - val_loss: 0.4716 - val_accuracy: 0.2034\n",
            "Epoch 29/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4096 - accuracy: 0.2110 - val_loss: 0.4718 - val_accuracy: 0.2033\n",
            "Epoch 30/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4084 - accuracy: 0.2114 - val_loss: 0.4723 - val_accuracy: 0.2033\n",
            "Epoch 31/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4072 - accuracy: 0.2117 - val_loss: 0.4737 - val_accuracy: 0.2029\n",
            "Epoch 32/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4061 - accuracy: 0.2120 - val_loss: 0.4736 - val_accuracy: 0.2032\n",
            "Epoch 33/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4051 - accuracy: 0.2124 - val_loss: 0.4748 - val_accuracy: 0.2029\n",
            "Epoch 34/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4041 - accuracy: 0.2126 - val_loss: 0.4737 - val_accuracy: 0.2031\n",
            "Epoch 35/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.4031 - accuracy: 0.2129 - val_loss: 0.4756 - val_accuracy: 0.2025\n",
            "Epoch 36/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4022 - accuracy: 0.2132 - val_loss: 0.4761 - val_accuracy: 0.2022\n",
            "Epoch 37/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4013 - accuracy: 0.2134 - val_loss: 0.4770 - val_accuracy: 0.2027\n",
            "Epoch 38/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4005 - accuracy: 0.2137 - val_loss: 0.4771 - val_accuracy: 0.2020\n",
            "Epoch 39/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3996 - accuracy: 0.2139 - val_loss: 0.4784 - val_accuracy: 0.2024\n",
            "Epoch 40/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3988 - accuracy: 0.2142 - val_loss: 0.4790 - val_accuracy: 0.2023\n",
            "Epoch 41/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3996 - accuracy: 0.2140 - val_loss: 0.4795 - val_accuracy: 0.2023\n",
            "Epoch 42/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3974 - accuracy: 0.2146 - val_loss: 0.4801 - val_accuracy: 0.2019\n",
            "Epoch 43/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3966 - accuracy: 0.2149 - val_loss: 0.4817 - val_accuracy: 0.2017\n",
            "Epoch 44/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3959 - accuracy: 0.2150 - val_loss: 0.4817 - val_accuracy: 0.2016\n",
            "Epoch 45/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3953 - accuracy: 0.2152 - val_loss: 0.4819 - val_accuracy: 0.2016\n",
            "Epoch 46/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3946 - accuracy: 0.2154 - val_loss: 0.4830 - val_accuracy: 0.2015\n",
            "Epoch 47/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3940 - accuracy: 0.2156 - val_loss: 0.4834 - val_accuracy: 0.2016\n",
            "Epoch 48/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3933 - accuracy: 0.2159 - val_loss: 0.4835 - val_accuracy: 0.2018\n",
            "Epoch 49/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3927 - accuracy: 0.2159 - val_loss: 0.4839 - val_accuracy: 0.2016\n",
            "Epoch 50/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3921 - accuracy: 0.2162 - val_loss: 0.4850 - val_accuracy: 0.2012\n",
            "Epoch 51/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3916 - accuracy: 0.2163 - val_loss: 0.4852 - val_accuracy: 0.2018\n",
            "Epoch 52/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3911 - accuracy: 0.2165 - val_loss: 0.4856 - val_accuracy: 0.2012\n",
            "Epoch 53/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3905 - accuracy: 0.2168 - val_loss: 0.4865 - val_accuracy: 0.2014\n",
            "Epoch 54/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3900 - accuracy: 0.2169 - val_loss: 0.4863 - val_accuracy: 0.2012\n",
            "Epoch 55/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3894 - accuracy: 0.2171 - val_loss: 0.4870 - val_accuracy: 0.2014\n",
            "Epoch 56/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3889 - accuracy: 0.2172 - val_loss: 0.4879 - val_accuracy: 0.2009\n",
            "Epoch 57/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3885 - accuracy: 0.2174 - val_loss: 0.4881 - val_accuracy: 0.2012\n",
            "Epoch 58/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3880 - accuracy: 0.2174 - val_loss: 0.4902 - val_accuracy: 0.2006\n",
            "Epoch 59/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3876 - accuracy: 0.2176 - val_loss: 0.4896 - val_accuracy: 0.2007\n",
            "Epoch 60/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3873 - accuracy: 0.2177 - val_loss: 0.4893 - val_accuracy: 0.2010\n",
            "Epoch 61/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3868 - accuracy: 0.2177 - val_loss: 0.4902 - val_accuracy: 0.2008\n",
            "Epoch 62/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3864 - accuracy: 0.2180 - val_loss: 0.4906 - val_accuracy: 0.2005\n",
            "Epoch 63/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3860 - accuracy: 0.2180 - val_loss: 0.4904 - val_accuracy: 0.2007\n",
            "Epoch 64/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3856 - accuracy: 0.2182 - val_loss: 0.4907 - val_accuracy: 0.2001\n",
            "Epoch 65/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3853 - accuracy: 0.2183 - val_loss: 0.4913 - val_accuracy: 0.2006\n",
            "Epoch 66/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3849 - accuracy: 0.2185 - val_loss: 0.4913 - val_accuracy: 0.2008\n",
            "Epoch 67/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3845 - accuracy: 0.2185 - val_loss: 0.4919 - val_accuracy: 0.2005\n",
            "Epoch 68/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3841 - accuracy: 0.2186 - val_loss: 0.4924 - val_accuracy: 0.2004\n",
            "Epoch 69/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3838 - accuracy: 0.2188 - val_loss: 0.4934 - val_accuracy: 0.2000\n",
            "Epoch 70/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3834 - accuracy: 0.2188 - val_loss: 0.4927 - val_accuracy: 0.2003\n",
            "Epoch 71/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3831 - accuracy: 0.2189 - val_loss: 0.4934 - val_accuracy: 0.2004\n",
            "Epoch 72/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3828 - accuracy: 0.2190 - val_loss: 0.4927 - val_accuracy: 0.2004\n",
            "Epoch 73/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3825 - accuracy: 0.2192 - val_loss: 0.4939 - val_accuracy: 0.2005\n",
            "Epoch 74/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3821 - accuracy: 0.2193 - val_loss: 0.4943 - val_accuracy: 0.2004\n",
            "Epoch 75/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3818 - accuracy: 0.2194 - val_loss: 0.4955 - val_accuracy: 0.2001\n",
            "Epoch 76/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3815 - accuracy: 0.2196 - val_loss: 0.4954 - val_accuracy: 0.2000\n",
            "Epoch 77/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3812 - accuracy: 0.2196 - val_loss: 0.4951 - val_accuracy: 0.2001\n",
            "Epoch 78/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3809 - accuracy: 0.2197 - val_loss: 0.4953 - val_accuracy: 0.2001\n",
            "Epoch 79/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3806 - accuracy: 0.2198 - val_loss: 0.4968 - val_accuracy: 0.1998\n",
            "Epoch 80/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3803 - accuracy: 0.2199 - val_loss: 0.4962 - val_accuracy: 0.2002\n",
            "Epoch 81/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3800 - accuracy: 0.2199 - val_loss: 0.4972 - val_accuracy: 0.1998\n",
            "Epoch 82/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3797 - accuracy: 0.2202 - val_loss: 0.4959 - val_accuracy: 0.1999\n",
            "Epoch 83/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3794 - accuracy: 0.2201 - val_loss: 0.4980 - val_accuracy: 0.1998\n",
            "Epoch 84/100\n",
            "905/905 [==============================] - 23s 25ms/step - loss: 0.3792 - accuracy: 0.2202 - val_loss: 0.4978 - val_accuracy: 0.1998\n",
            "Epoch 85/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3789 - accuracy: 0.2203 - val_loss: 0.4973 - val_accuracy: 0.1996\n",
            "Epoch 86/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3786 - accuracy: 0.2204 - val_loss: 0.4977 - val_accuracy: 0.1997\n",
            "Epoch 87/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3784 - accuracy: 0.2206 - val_loss: 0.4986 - val_accuracy: 0.1996\n",
            "Epoch 88/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3780 - accuracy: 0.2206 - val_loss: 0.4987 - val_accuracy: 0.1992\n",
            "Epoch 89/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3778 - accuracy: 0.2208 - val_loss: 0.4991 - val_accuracy: 0.1993\n",
            "Epoch 90/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3775 - accuracy: 0.2207 - val_loss: 0.4993 - val_accuracy: 0.1995\n",
            "Epoch 91/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3772 - accuracy: 0.2209 - val_loss: 0.4998 - val_accuracy: 0.1995\n",
            "Epoch 92/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3769 - accuracy: 0.2210 - val_loss: 0.5003 - val_accuracy: 0.1995\n",
            "Epoch 93/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3767 - accuracy: 0.2211 - val_loss: 0.5000 - val_accuracy: 0.1991\n",
            "Epoch 94/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3764 - accuracy: 0.2211 - val_loss: 0.5008 - val_accuracy: 0.1993\n",
            "Epoch 95/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3762 - accuracy: 0.2212 - val_loss: 0.5010 - val_accuracy: 0.1994\n",
            "Epoch 96/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3759 - accuracy: 0.2213 - val_loss: 0.5011 - val_accuracy: 0.1991\n",
            "Epoch 97/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3757 - accuracy: 0.2214 - val_loss: 0.5012 - val_accuracy: 0.1990\n",
            "Epoch 98/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3755 - accuracy: 0.2215 - val_loss: 0.5015 - val_accuracy: 0.1990\n",
            "Epoch 99/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3752 - accuracy: 0.2215 - val_loss: 0.5020 - val_accuracy: 0.1987\n",
            "Epoch 100/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3750 - accuracy: 0.2217 - val_loss: 0.5025 - val_accuracy: 0.1988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f06ee3616a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJhfUd1fxhr"
      },
      "source": [
        "**Saving the Sequence to Sequence Chatbot Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7neRc0DgcB38"
      },
      "source": [
        "#@ Saving the Sequence to Sequence Network Model:\n",
        "model_structure = model.to_json()\n",
        "with open(\"chatbot_model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"chatbot_model.h5\")\n",
        "print(\"Model saved successful!!\")\n",
        "\n",
        "model.load_weights(\"chatbot_model.h5\")                         # Loading the saved Model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5lEyWE0SOmw"
      },
      "source": [
        "**Assembling the Model for Sequence Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2gQBZG8XStP"
      },
      "source": [
        "#@ Creating the Response Generator Model:\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "thought_input = [Input(shape=(num_neurons,)), Input(shape=(num_neurons,))]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=thought_input)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#@ Creating the Model:\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_inputs] + thought_input,\n",
        "    outputs=[decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVpilaALptgE"
      },
      "source": [
        "### **Predicting the Sequence**\n",
        "* I will define a Function for generating the Response of the Chatbot. This Function is the heart of Response Generation of the Chatbot which accepts one hot encoded input sequence, generates the Thought Vector and the Thought Vector generates the appropriate response by using the **Trained Network**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljiCePksBqX"
      },
      "source": [
        "#@ Building the Character Based Translator:\n",
        "def decode_sequence(input_seq):\n",
        "  #@ Generating the Thought Vector:\n",
        "  thought = encoder_model.predict(input_seq)\n",
        "\n",
        "  target_seq = np.zeros((1, 1, output_vocab_size))                          # Initializing it as a Zero Tensor.\n",
        "  target_seq[0, 0, target_token_index[stop_token]] = 1.                           # First Input Token to the decoder is the input token.\n",
        "\n",
        "  stop_condition = False\n",
        "  generated_sequence = \"\"\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + thought)     # Passing the generated Token and latest state to the Decoder.\n",
        "    generated_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "    generated_char = reverse_target_char_index[generated_token_idx]\n",
        "    generated_sequence += generated_char\n",
        "    if (generated_char == stop_token or \n",
        "        len(generated_sequence) > max_decoder_seq_length):\n",
        "      stop_condition = True                                                 # Setting the condition to True will stop the Loop.\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, output_vocab_size))\n",
        "    target_seq[0, 0, generated_token_idx] = 1.\n",
        "    thought = [h, c]                                                        # Updating the Thought Vector.\n",
        "\n",
        "  return generated_sequence"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDC_1pd8uYX"
      },
      "source": [
        "**Generating the Response**\n",
        "* Now, I will define a helper function to convert the Input String into a reply for the Chatbot to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKVIBZp4WVM"
      },
      "source": [
        "def Response(input_text):\n",
        "  input_seq = np.zeros((1, max_encoder_seq_length, input_vocab_size), dtype=\"float32\")\n",
        "  for t, char in enumerate(input_text):\n",
        "    input_seq[0, t, input_token_index[char]] = 1.\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(\"T2 Reply:\", decoded_sentence)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPrjC-3qD1td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f10284-e4c3-4668-a3b5-849c2cc3cfc8"
      },
      "source": [
        "Response(\"do you sing a song?\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T2 Reply: not at all.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU8DsuFrsiBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0f5b6d-0e1b-4691-e81a-7b0c0a8540c5"
      },
      "source": [
        "Response(\"do you like coffee?\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T2 Reply: i don't know. i haven't seen the way you want to get the disturbers and then we can get the general to \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leoj79_q8Ufp",
        "outputId": "67a52965-6c46-4d26-f338-2456f834f999"
      },
      "source": [
        "Response(\"do you like football?\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T2 Reply: no.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}