{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtgdRkCNxbx1"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMeD70JilyF"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmJEcP40Wput"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3bnM_MSx1YI"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "# !pip install nlpia                                                       # Downloading the NLPIA Package.\n",
        "\n",
        "import numpy as np                                                         # Module for matrix multiplication.\n",
        "from nlpia.loaders import get_data \n",
        "import os, re\n",
        "from random import shuffle                                                 # Module for shuffling the Dataset.\n",
        "from IPython.display import display\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9pYizaYRFF"
      },
      "source": [
        "**Getting the Data**\n",
        "* I will use the [**Cornell Movie Dialog Dataset**](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). Using the entire Cornell Movie Dialog Dataset can be computationally intensive because a few sequences have more than 2000 tokens. I will use **NLPIA** Package to load the Cornell Movie Dialog Dataset and I will pre process the Dialog Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-PreqwYDFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1ed8fab0-1a30-4f1c-b589-f4db9272656d"
      },
      "source": [
        "#@ Getting the Data:\n",
        "df = get_data(\"moviedialog\")                                                # Accessing the Cornell Movie Dialog Corpus.\n",
        "\n",
        "#@ Processing the Data:\n",
        "input_texts = []                                                            # The array holds the input text from the Corpus.\n",
        "target_texts = []                                                           # The array holds the target text from the Corpus.\n",
        "input_vocabulary = set()                                                    # Holds the seen characters in the input text.\n",
        "output_vocabulary = set()                                                   # Holds the seen characters in the target txt.\n",
        "\n",
        "start_token = \"\\t\"                                                          # Target sequence is annotated with start Token.\n",
        "stop_token = \"\\n\"                                                           # Target sequence is annotated with sop Token.\n",
        "# max_training_samples = min(25000, len(df) - 1)                              # Defines the lines used for Training.\n",
        "\n",
        "for input_text, target_text in zip(df.statement, df.reply):\n",
        "  target_text = start_token + target_text + stop_token                      # The Target Text needs to be wrapped with start and stop tokens.\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  \n",
        "  #@ Compiling the Vocabulary set:\n",
        "  for char in input_text:\n",
        "    if char not in input_vocabulary:\n",
        "      input_vocabulary.add(char)\n",
        "  \n",
        "  for char in target_text:\n",
        "    if char not in output_vocabulary:\n",
        "      output_vocabulary.add(char)\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "\n",
        "display(f\"Number of samples: {len(input_texts)}\")\n",
        "display(f\"Number of unique Input Tokens: {len(input_vocabulary)}\")\n",
        "display(f\"Number of unique Output Tokens: {len(output_vocabulary)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of samples: 64350'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of unique Input Tokens: 44'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Number of unique Output Tokens: 46'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29qMwBjZjPXc"
      },
      "source": [
        "**Building the Character Dictionary**\n",
        "* I will convert each characters of the Input and Target Texts into one hot vectors that represent each characters. In order to generate one hot vectors I will generate token dictionaries where every character is mapped to an index. I will also generate the reverse dictionaries which will be used to convert generated index into characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrnA7fjMb4vd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d0ae3f2e-7204-429e-8389-7f4cdefdf89b"
      },
      "source": [
        "#@ Sorting the List of Characters:\n",
        "input_vocabulary = sorted(input_vocabulary)\n",
        "output_vocabulary = sorted(output_vocabulary)\n",
        "\n",
        "#@ Calculating the Maximum number of Unique Characters:\n",
        "input_vocab_size = len(input_vocabulary)\n",
        "output_vocab_size = len(output_vocabulary)\n",
        "\n",
        "#@ Determining the Maximum number of Sequence Tokens:\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "#@ Creating the Token Dictionaries:\n",
        "input_token_index = dict([(char, i) for i,char in enumerate(input_vocabulary)])\n",
        "target_token_index = dict([(char, i) for i,char in enumerate(output_vocabulary)])\n",
        "\n",
        "#@ Creating the Reverse Token Dictionaries:\n",
        "reverse_input_char_index = dict((i, char) for char,i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char,i in target_token_index.items())\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "display(f\"Maximum sequence length for Inputs: {max_encoder_seq_length}\")\n",
        "display(f\"Maximum sequence length for Outputs: {max_decoder_seq_length}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maximum sequence length for Inputs: 100'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Maximum sequence length for Outputs: 102'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fIX28MrwXO"
      },
      "source": [
        "**Generating One Hot Encoded Training sets**\n",
        "* Now, I will convert the input and target text into one hot Encoded Tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkpcZ8YssSOe"
      },
      "source": [
        "#@ Creating character sequence Encoder and Decoder Training Set:\n",
        "\n",
        "#@ Initializing the Tensors with zeros:\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, input_vocab_size), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype=\"float32\")\n",
        "\n",
        "#@ Looping over the Training Samples:\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "  #@ Looping over each character of each Samples:\n",
        "  for t, char in enumerate(input_text):\n",
        "    encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "  for t, char in enumerate(target_text):\n",
        "    decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "    if t > 0:\n",
        "      decoder_target_data[i, t-1, target_token_index[char]] = 1.\n",
        "  # decoder_input_data[i, t+1:, target_token_index[\" \"]] = 1.\n",
        "  # decoder_target_data[i, t:, target_token_index[\" \"]] = 1."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-6_CLPRzzLA"
      },
      "source": [
        "### **Sequence to Sequence Chatbot**\n",
        "* I have completed all the Training set preparations by performing the tasks such as Converting the preprocessed Corpus into Input and Target Samples and creating Index Dictionaries and converting the Samples into One hot Tensors. Now, I will train the Sequence to sequence Chatbot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbDzIUUk5JDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d258a1-e748-4659-8f0e-77c6504415af"
      },
      "source": [
        "#@ Parameters of LSTM Neural Networks:\n",
        "batch_size = 64                                 # Number of samples shown to the network before updating the weights.\n",
        "epochs = 100                                    # Number of times for passing the Training.\n",
        "num_neurons = 256                               # Setting the number of neuron dimensions to 256.\n",
        "\n",
        "#@ Sequence to Sequence Encoder Decoder Network:\n",
        "checkpoint_filepath = \"/content/chatbotmodel\"\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = checkpoint_filepath,\n",
        "    save_weights_only = True,\n",
        "    monitor = \"loss\",\n",
        "    mode = \"min\",\n",
        "    save_best_only = True\n",
        ")\n",
        "\n",
        "#@ Creating the Thought Encoder using Keras Functional API:\n",
        "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
        "encoder = LSTM(num_neurons, return_state=True)                                          # Returning the internal state of LSTM.\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs) \n",
        "encoder_states = [state_h, state_c]                                                     # First value of LSTM is the Output.\n",
        "\n",
        "#@ Creating the Thought Decoder using Keras Functional API:\n",
        "decoder_inputs = Input(shape=(None, output_vocab_size))\n",
        "decoder_lstm = LSTM(\n",
        "    num_neurons, return_sequences=True, return_state=True\n",
        ")\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)      # Passing initial state to the LSTM Layer.\n",
        "decoder_dense = Dense(output_vocab_size, activation=\"softmax\")                          \n",
        "decoder_outputs = decoder_dense(decoder_outputs)                                        # Passing the output to the Softmax Layer.\n",
        "\n",
        "#@ Creating the Sequence to Sequence Neural Network Model:\n",
        "model = Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs\n",
        ")\n",
        "\n",
        "#@ Compiling the Sequence to Sequence Neural Network Model:\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"categorical_crossentropy\",                                                    # Using Categorical Crossentropy.\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#@ Training the Sequence to Sequence Neural Network Model:\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data], \n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,                                                              # 10% of Samples are splitted for Validation.\n",
        "    callbacks = [model_checkpoint_callback]\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "905/905 [==============================] - 25s 27ms/step - loss: 0.7592 - accuracy: 0.1202 - val_loss: 0.6470 - val_accuracy: 0.1546\n",
            "Epoch 2/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.5908 - accuracy: 0.1623 - val_loss: 0.5738 - val_accuracy: 0.1741\n",
            "Epoch 3/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.5381 - accuracy: 0.1762 - val_loss: 0.5385 - val_accuracy: 0.1837\n",
            "Epoch 4/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.5105 - accuracy: 0.1836 - val_loss: 0.5187 - val_accuracy: 0.1891\n",
            "Epoch 5/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4930 - accuracy: 0.1883 - val_loss: 0.5049 - val_accuracy: 0.1932\n",
            "Epoch 6/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4806 - accuracy: 0.1917 - val_loss: 0.4965 - val_accuracy: 0.1952\n",
            "Epoch 7/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4712 - accuracy: 0.1942 - val_loss: 0.4908 - val_accuracy: 0.1970\n",
            "Epoch 8/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4636 - accuracy: 0.1963 - val_loss: 0.4852 - val_accuracy: 0.1984\n",
            "Epoch 9/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4576 - accuracy: 0.1979 - val_loss: 0.4812 - val_accuracy: 0.1995\n",
            "Epoch 10/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4523 - accuracy: 0.1993 - val_loss: 0.4787 - val_accuracy: 0.2005\n",
            "Epoch 11/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4478 - accuracy: 0.2006 - val_loss: 0.4769 - val_accuracy: 0.2010\n",
            "Epoch 12/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4437 - accuracy: 0.2017 - val_loss: 0.4747 - val_accuracy: 0.2016\n",
            "Epoch 13/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4402 - accuracy: 0.2026 - val_loss: 0.4732 - val_accuracy: 0.2018\n",
            "Epoch 14/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4370 - accuracy: 0.2035 - val_loss: 0.4719 - val_accuracy: 0.2023\n",
            "Epoch 15/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4341 - accuracy: 0.2043 - val_loss: 0.4718 - val_accuracy: 0.2023\n",
            "Epoch 16/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4314 - accuracy: 0.2050 - val_loss: 0.4700 - val_accuracy: 0.2030\n",
            "Epoch 17/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4290 - accuracy: 0.2057 - val_loss: 0.4707 - val_accuracy: 0.2027\n",
            "Epoch 18/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4267 - accuracy: 0.2063 - val_loss: 0.4697 - val_accuracy: 0.2030\n",
            "Epoch 19/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4246 - accuracy: 0.2068 - val_loss: 0.4699 - val_accuracy: 0.2029\n",
            "Epoch 20/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4226 - accuracy: 0.2074 - val_loss: 0.4695 - val_accuracy: 0.2034\n",
            "Epoch 21/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4208 - accuracy: 0.2079 - val_loss: 0.4693 - val_accuracy: 0.2031\n",
            "Epoch 22/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4190 - accuracy: 0.2084 - val_loss: 0.4703 - val_accuracy: 0.2034\n",
            "Epoch 23/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4174 - accuracy: 0.2089 - val_loss: 0.4695 - val_accuracy: 0.2039\n",
            "Epoch 24/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4159 - accuracy: 0.2092 - val_loss: 0.4692 - val_accuracy: 0.2037\n",
            "Epoch 25/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4143 - accuracy: 0.2097 - val_loss: 0.4699 - val_accuracy: 0.2035\n",
            "Epoch 26/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4129 - accuracy: 0.2101 - val_loss: 0.4706 - val_accuracy: 0.2031\n",
            "Epoch 27/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4117 - accuracy: 0.2105 - val_loss: 0.4719 - val_accuracy: 0.2032\n",
            "Epoch 28/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4104 - accuracy: 0.2108 - val_loss: 0.4716 - val_accuracy: 0.2032\n",
            "Epoch 29/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4092 - accuracy: 0.2112 - val_loss: 0.4722 - val_accuracy: 0.2038\n",
            "Epoch 30/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4080 - accuracy: 0.2115 - val_loss: 0.4731 - val_accuracy: 0.2030\n",
            "Epoch 31/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4069 - accuracy: 0.2119 - val_loss: 0.4735 - val_accuracy: 0.2034\n",
            "Epoch 32/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4058 - accuracy: 0.2121 - val_loss: 0.4742 - val_accuracy: 0.2029\n",
            "Epoch 33/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4048 - accuracy: 0.2125 - val_loss: 0.4745 - val_accuracy: 0.2029\n",
            "Epoch 34/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4038 - accuracy: 0.2128 - val_loss: 0.4747 - val_accuracy: 0.2029\n",
            "Epoch 35/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.4038 - accuracy: 0.2130 - val_loss: 0.4751 - val_accuracy: 0.2030\n",
            "Epoch 36/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4021 - accuracy: 0.2133 - val_loss: 0.4762 - val_accuracy: 0.2030\n",
            "Epoch 37/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4011 - accuracy: 0.2135 - val_loss: 0.4768 - val_accuracy: 0.2024\n",
            "Epoch 38/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.4003 - accuracy: 0.2138 - val_loss: 0.4770 - val_accuracy: 0.2029\n",
            "Epoch 39/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3997 - accuracy: 0.2140 - val_loss: 0.4782 - val_accuracy: 0.2023\n",
            "Epoch 40/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3987 - accuracy: 0.2144 - val_loss: 0.4785 - val_accuracy: 0.2026\n",
            "Epoch 41/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3979 - accuracy: 0.2146 - val_loss: 0.4786 - val_accuracy: 0.2026\n",
            "Epoch 42/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3982 - accuracy: 0.2146 - val_loss: 0.4806 - val_accuracy: 0.2021\n",
            "Epoch 43/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3981 - accuracy: 0.2146 - val_loss: 0.4803 - val_accuracy: 0.2019\n",
            "Epoch 44/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3965 - accuracy: 0.2150 - val_loss: 0.4806 - val_accuracy: 0.2024\n",
            "Epoch 45/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3955 - accuracy: 0.2152 - val_loss: 0.4815 - val_accuracy: 0.2020\n",
            "Epoch 46/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3958 - accuracy: 0.2151 - val_loss: 0.4821 - val_accuracy: 0.2023\n",
            "Epoch 47/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3947 - accuracy: 0.2154 - val_loss: 0.4812 - val_accuracy: 0.2020\n",
            "Epoch 48/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3940 - accuracy: 0.2157 - val_loss: 0.4821 - val_accuracy: 0.2019\n",
            "Epoch 49/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3931 - accuracy: 0.2159 - val_loss: 0.4836 - val_accuracy: 0.2016\n",
            "Epoch 50/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3925 - accuracy: 0.2161 - val_loss: 0.4840 - val_accuracy: 0.2018\n",
            "Epoch 51/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3918 - accuracy: 0.2163 - val_loss: 0.4832 - val_accuracy: 0.2018\n",
            "Epoch 52/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3912 - accuracy: 0.2165 - val_loss: 0.4846 - val_accuracy: 0.2017\n",
            "Epoch 53/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3909 - accuracy: 0.2166 - val_loss: 0.4850 - val_accuracy: 0.2016\n",
            "Epoch 54/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3903 - accuracy: 0.2168 - val_loss: 0.4862 - val_accuracy: 0.2016\n",
            "Epoch 55/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3898 - accuracy: 0.2170 - val_loss: 0.4862 - val_accuracy: 0.2016\n",
            "Epoch 56/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3894 - accuracy: 0.2170 - val_loss: 0.4873 - val_accuracy: 0.2012\n",
            "Epoch 57/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3889 - accuracy: 0.2171 - val_loss: 0.4871 - val_accuracy: 0.2013\n",
            "Epoch 58/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3885 - accuracy: 0.2173 - val_loss: 0.4876 - val_accuracy: 0.2010\n",
            "Epoch 59/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3882 - accuracy: 0.2174 - val_loss: 0.4876 - val_accuracy: 0.2014\n",
            "Epoch 60/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3878 - accuracy: 0.2175 - val_loss: 0.4880 - val_accuracy: 0.2014\n",
            "Epoch 61/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3872 - accuracy: 0.2177 - val_loss: 0.4884 - val_accuracy: 0.2012\n",
            "Epoch 62/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3870 - accuracy: 0.2178 - val_loss: 0.4890 - val_accuracy: 0.2008\n",
            "Epoch 63/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3867 - accuracy: 0.2179 - val_loss: 0.4892 - val_accuracy: 0.2013\n",
            "Epoch 64/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3863 - accuracy: 0.2179 - val_loss: 0.4892 - val_accuracy: 0.2012\n",
            "Epoch 65/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3861 - accuracy: 0.2180 - val_loss: 0.4911 - val_accuracy: 0.2007\n",
            "Epoch 66/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3856 - accuracy: 0.2181 - val_loss: 0.4900 - val_accuracy: 0.2008\n",
            "Epoch 67/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3854 - accuracy: 0.2183 - val_loss: 0.4904 - val_accuracy: 0.2010\n",
            "Epoch 68/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3850 - accuracy: 0.2183 - val_loss: 0.4902 - val_accuracy: 0.2014\n",
            "Epoch 69/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3847 - accuracy: 0.2184 - val_loss: 0.4912 - val_accuracy: 0.2008\n",
            "Epoch 70/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3844 - accuracy: 0.2185 - val_loss: 0.4907 - val_accuracy: 0.2010\n",
            "Epoch 71/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3842 - accuracy: 0.2185 - val_loss: 0.4910 - val_accuracy: 0.2008\n",
            "Epoch 72/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3841 - accuracy: 0.2186 - val_loss: 0.4929 - val_accuracy: 0.2001\n",
            "Epoch 73/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3840 - accuracy: 0.2186 - val_loss: 0.4925 - val_accuracy: 0.2008\n",
            "Epoch 74/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3834 - accuracy: 0.2188 - val_loss: 0.4912 - val_accuracy: 0.2013\n",
            "Epoch 75/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3833 - accuracy: 0.2189 - val_loss: 0.4927 - val_accuracy: 0.2008\n",
            "Epoch 76/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3832 - accuracy: 0.2189 - val_loss: 0.4927 - val_accuracy: 0.2004\n",
            "Epoch 77/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3827 - accuracy: 0.2191 - val_loss: 0.4935 - val_accuracy: 0.2005\n",
            "Epoch 78/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3826 - accuracy: 0.2191 - val_loss: 0.4932 - val_accuracy: 0.2009\n",
            "Epoch 79/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3903 - accuracy: 0.2174 - val_loss: 0.4928 - val_accuracy: 0.2010\n",
            "Epoch 80/100\n",
            "905/905 [==============================] - 23s 26ms/step - loss: 0.3847 - accuracy: 0.2185 - val_loss: 0.4927 - val_accuracy: 0.2010\n",
            "Epoch 81/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3823 - accuracy: 0.2191 - val_loss: 0.4938 - val_accuracy: 0.2003\n",
            "Epoch 82/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3820 - accuracy: 0.2192 - val_loss: 0.4933 - val_accuracy: 0.2005\n",
            "Epoch 83/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3816 - accuracy: 0.2195 - val_loss: 0.4940 - val_accuracy: 0.2006\n",
            "Epoch 84/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3813 - accuracy: 0.2194 - val_loss: 0.4941 - val_accuracy: 0.2007\n",
            "Epoch 85/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3811 - accuracy: 0.2195 - val_loss: 0.4954 - val_accuracy: 0.2004\n",
            "Epoch 86/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3809 - accuracy: 0.2195 - val_loss: 0.4946 - val_accuracy: 0.2005\n",
            "Epoch 87/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3806 - accuracy: 0.2197 - val_loss: 0.4948 - val_accuracy: 0.2005\n",
            "Epoch 88/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3804 - accuracy: 0.2197 - val_loss: 0.4958 - val_accuracy: 0.2002\n",
            "Epoch 89/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3803 - accuracy: 0.2197 - val_loss: 0.4961 - val_accuracy: 0.2003\n",
            "Epoch 90/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3801 - accuracy: 0.2199 - val_loss: 0.4964 - val_accuracy: 0.2003\n",
            "Epoch 91/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3800 - accuracy: 0.2198 - val_loss: 0.4959 - val_accuracy: 0.2003\n",
            "Epoch 92/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3801 - accuracy: 0.2198 - val_loss: 0.4967 - val_accuracy: 0.1999\n",
            "Epoch 93/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3798 - accuracy: 0.2199 - val_loss: 0.4963 - val_accuracy: 0.2003\n",
            "Epoch 94/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3794 - accuracy: 0.2201 - val_loss: 0.4964 - val_accuracy: 0.2002\n",
            "Epoch 95/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3792 - accuracy: 0.2200 - val_loss: 0.4966 - val_accuracy: 0.2005\n",
            "Epoch 96/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3791 - accuracy: 0.2201 - val_loss: 0.4972 - val_accuracy: 0.2003\n",
            "Epoch 97/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3789 - accuracy: 0.2202 - val_loss: 0.4975 - val_accuracy: 0.2001\n",
            "Epoch 98/100\n",
            "905/905 [==============================] - 24s 27ms/step - loss: 0.3787 - accuracy: 0.2202 - val_loss: 0.4973 - val_accuracy: 0.2004\n",
            "Epoch 99/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3785 - accuracy: 0.2203 - val_loss: 0.4974 - val_accuracy: 0.2000\n",
            "Epoch 100/100\n",
            "905/905 [==============================] - 24s 26ms/step - loss: 0.3784 - accuracy: 0.2203 - val_loss: 0.4969 - val_accuracy: 0.2003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6b4a6aa9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJhfUd1fxhr"
      },
      "source": [
        "**Saving the Sequence to Sequence Chatbot Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7neRc0DgcB38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75752f84-fc8d-446c-c98e-f8081c550414"
      },
      "source": [
        "#@ Saving the Sequence to Sequence Network Model:\n",
        "model.load_weights(checkpoint_filepath)\n",
        "model_structure = model.to_json()\n",
        "with open(\"chatbot_model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"chatbot_model.h5\")\n",
        "print(\"Model saved successful!!\")\n",
        "\n",
        "model.load_weights(\"chatbot_model.h5\")                         # Loading the saved Model."
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved successful!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5lEyWE0SOmw"
      },
      "source": [
        "**Assembling the Model for Sequence Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2gQBZG8XStP"
      },
      "source": [
        "#@ Creating the Response Generator Model:\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "thought_input = [Input(shape=(num_neurons,)), Input(shape=(num_neurons,))]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=thought_input)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#@ Creating the Model:\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_inputs] + thought_input,\n",
        "    outputs=[decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVpilaALptgE"
      },
      "source": [
        "### **Predicting the Sequence**\n",
        "* I will define a Function for generating the Response of the Chatbot. This Function is the heart of Response Generation of the Chatbot which accepts one hot encoded input sequence, generates the Thought Vector and the Thought Vector generates the appropriate response by using the **Trained Network**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljiCePksBqX"
      },
      "source": [
        "#@ Building the Character Based Translator:\n",
        "def decode_sequence(input_seq):\n",
        "  #@ Generating the Thought Vector:\n",
        "  thought = encoder_model.predict(input_seq)\n",
        "\n",
        "  target_seq = np.zeros((1, 1, output_vocab_size))                          # Initializing it as a Zero Tensor.\n",
        "  target_seq[0, 0, target_token_index[stop_token]] = 1.                     # First Input Token to the decoder is the input token.\n",
        "\n",
        "  stop_condition = False\n",
        "  generated_sequence = \"\"\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + thought)     # Passing the generated Token and latest state to the Decoder.\n",
        "    generated_token_idx = np.argmax(output_tokens[0, -1, :])\n",
        "    generated_char = reverse_target_char_index[generated_token_idx]\n",
        "    generated_sequence += generated_char\n",
        "    if (generated_char == stop_token or \n",
        "        len(generated_sequence) > max_decoder_seq_length):\n",
        "      stop_condition = True                                                 # Setting the condition to True will stop the Loop.\n",
        "    \n",
        "    target_seq = np.zeros((1, 1, output_vocab_size))\n",
        "    target_seq[0, 0, generated_token_idx] = 1.\n",
        "    thought = [h, c]                                                        # Updating the Thought Vector.\n",
        "\n",
        "  return generated_sequence"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDC_1pd8uYX"
      },
      "source": [
        "**Generating the Response**\n",
        "* Now, I will define a helper function to convert the Input String into a reply for the Chatbot to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbXf64bqUW47",
        "outputId": "5afd73eb-bfae-49ff-c847-44891e7a4653"
      },
      "source": [
        "class ChatBot:\n",
        "  negative_responses = (\"No\", \"Not a chance\", \"Sorry\")\n",
        "  exit_commands = (\"Quit\", \"Pause\", \"Exit\", \"Goodbye\", \"Bye\", \"Later\", \"Stop\")\n",
        "\n",
        "#@ Starting the Conversation:\n",
        "  def start_chat(self):\n",
        "    user_response = input(\"Hi, I'm a T2 Chatbot. Would you like to chat with me?\\n\")\n",
        "    if user_response in self.negative_responses:\n",
        "      print(\"Okay, Have a great day!\")\n",
        "      return\n",
        "    self.chat(user_response)\n",
        "\n",
        "#@ Conversation:\n",
        "  def chat(self, reply):\n",
        "    while not self.make_exit(reply):\n",
        "      reply = input(self.generate_response(reply) + \"\\n\")\n",
        "    \n",
        "  #@ Convert user input into matries:\n",
        "  def string_to_matrix(self, user_input):\n",
        "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
        "    user_input_matrix = np.zeros((1, max_encoder_seq_length, input_vocab_size), \n",
        "                                 dtype='float32')\n",
        "    for timestep, token in enumerate(tokens):\n",
        "      if token in input_token_index:\n",
        "        user_input_matrix[0, timestep, input_token_index[token]] = 1.\n",
        "    return user_input_matrix\n",
        "  \n",
        "  #@ Creating the Response:\n",
        "  def generate_response(self, user_input):\n",
        "    input_matrix = self.string_to_matrix(user_input)\n",
        "    chatbot_response = decode_sequence(input_matrix)\n",
        "    chatbot_response = chatbot_response.replace(\"<START>\", \"\")\n",
        "    chatbot_response = chatbot_response.replace(\"<END>\", \"\")\n",
        "    return chatbot_response\n",
        "\n",
        "#@ Checking the Exit Commands:\n",
        "  def make_exit(self, reply):\n",
        "    for exit_command in self.exit_commands:\n",
        "      if exit_command in reply:\n",
        "        print(\"Okay, Have a great day!\")\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "chatbot = ChatBot()                                                                 # Instantiating the Chatbot.                        \n",
        "chatbot.start_chat()                                                                # Running the Chatbot."
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi, I'm a T2 Chatbot. Would you like to chat with me?\n",
            "No\n",
            "Okay, Have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDKVIBZp4WVM"
      },
      "source": [
        "def Response(input_text):\n",
        "  input_seq = np.zeros((1, max_encoder_seq_length, input_vocab_size), dtype=\"float32\")\n",
        "  for t, char in enumerate(input_text):\n",
        "    input_seq[0, t, input_token_index[char]] = 1.\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(\"T2 Reply:\", decoded_sentence)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUnFHaZxj1Ic",
        "outputId": "1f6be7be-e4a9-4e2a-8602-60446e9582df"
      },
      "source": [
        "Response(\"what is the internet?\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T2 Reply: i don't know. i was all set.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lvLPFw4kmFk",
        "outputId": "4263d805-5fb2-4292-a4e3-55ddd17458cd"
      },
      "source": [
        "Response(\"do you like coffee?\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T2 Reply: yeah.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}